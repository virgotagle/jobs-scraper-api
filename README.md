# Jobs Scraper API

A FastAPI-based REST API for managing and querying job listings scraped from various job sites. This API provides endpoints for searching, filtering, and retrieving detailed job information stored in a SQLite database.

## Data Source 

This API consumes data generated by the [jobs-scraper](https://github.com/virgotagle/jobs-scraper) project - a modern, async web scraper that collects job listings from job sites like Seek.co.nz. The scraper uses Playwright with stealth mode, built-in rate limiting, and stores data in the `jobs.db` SQLite database that this API serves.

**Key features of the scraper:**
- üöÄ Async/await architecture with Playwright browser automation
- üïµÔ∏è Stealth browsing to avoid detection
- ‚è±Ô∏è Respectful rate limiting (2-4 second delays between requests)
- üóÑÔ∏è Automated SQLite storage with the same schema as this API
- üîß Extensible design for adding new job sites

To collect fresh job data, set up and run the jobs-scraper following its [installation guide](https://github.com/virgotagle/jobs-scraper#-installation).

## Architecture

### Technology Stack

- **Framework**: FastAPI 0.122.0+
- **Database**: SQLite with SQLAlchemy 2.0.44+
- **Validation**: Pydantic 2.12.5+
- **Python**: 3.12+
- **Testing**: pytest 8.3.4+

### Project Structure

```
jobs-scraper-api/
‚îú‚îÄ‚îÄ main.py                      # FastAPI application entry point
‚îú‚îÄ‚îÄ pyproject.toml               # Project dependencies and metadata
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py          # Database connection management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py            # SQLAlchemy ORM models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories.py      # Data access layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py           # Pydantic schemas for API
‚îÇ   ‚îî‚îÄ‚îÄ routers/
‚îÇ       ‚îî‚îÄ‚îÄ jobs.py              # Job listing API endpoints
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_api.py              # API endpoint tests (requires jobs.db)
    ‚îî‚îÄ‚îÄ test_repository.py       # Repository layer tests
```

## Data Model

### Job Listings (`job_listings` table)

Primary entity storing job listing information:

- `job_id` (PK): Unique job identifier
- `title`: Job title
- `job_details_url`: URL to full job posting
- `job_summary`: Brief job description
- `company_name`: Employer name
- `location`: Job location
- `country_code`: ISO country code
- `listing_date`: Date job was posted
- `salary_label`: Salary information (optional)
- `work_type`: Employment type (optional)
- `job_classification`: Primary job category (optional)
- `job_sub_classification`: Job subcategory (optional)
- `work_arrangements`: Remote/hybrid/office (optional)

### Job Details (`job_details` table)

Extended job information with 1:1 relationship to listings:

- `job_id` (PK, FK): References `job_listings.job_id`
- `status`: Current job status
- `is_expired`: Expiration flag
- `details`: Full job description
- `is_verified`: Verification status (optional)
- `expires_at`: Expiration timestamp (optional)

## API Endpoints

### Base URL
`/` - API root endpoint returning `{"message": "Jobs Scraper API", "version": "1.0.0"}`

### Job Endpoints (`/jobs`)

#### List Jobs
```
GET /jobs/
```
Query parameters:
- `job_classification` (optional): Filter by job category
- `job_sub_classification` (optional): Filter by subcategory
- `work_arrangements` (optional): Filter by work arrangement
- `skip` (default: 0): Pagination offset
- `limit` (default: 100, max: 100): Results per page

#### Get Job Details
```
GET /jobs/{job_id}
```
Returns full job information including extended details.

#### Search Jobs
```
GET /jobs/search
```
Query parameters:
- `keyword` (required): Search term (searches across title, summary, company, location, and details)
- `skip` (default: 0): Pagination offset
- `limit` (default: 100): Results per page

#### Get Classifications
```
GET /jobs/classifications
```
Returns list of all unique job classifications in database.

#### Get Sub-classifications
```
GET /jobs/sub-classifications
```
Returns list of all unique job sub-classifications.

#### Get Work Arrangements
```
GET /jobs/work-arrangements
```
Returns list of all unique work arrangement types.

## Installation

### Prerequisites

- Python 3.12 or higher
- uv package manager (recommended) or pip

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd jobs-scraper-api
```

2. Install dependencies:
```bash
# Using uv (recommended)
uv sync

# Or using pip
pip install -e .
```

3. Install development dependencies (already included with uv sync):
```bash
# Development dependencies are automatically installed with uv sync
# To install only production dependencies:
uv sync --no-dev
```

## Running the API

### Development Server

Start the FastAPI development server with auto-reload:

```bash
fastapi dev main.py
```

The API will be available at `http://localhost:8000`

### Production Server

```bash
fastapi run main.py
```

### Interactive API Documentation

Once running, access:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## Database

### Initialization

The database is automatically initialized on application startup using the lifespan context manager in `main.py`. By default, it uses a `jobs.db` SQLite file in the project root.

**Note**: The API expects an existing `jobs.db` file populated by the [jobs-scraper](https://github.com/virgotagle/jobs-scraper) project. If the database doesn't exist or is empty, the API will return empty results.

### Data Collection

To populate the database with job listings:

1. Clone and set up the jobs-scraper:
```bash
git clone https://github.com/virgotagle/jobs-scraper
cd jobs-scraper
uv sync
uv run playwright install
```

2. Run the scraper to collect jobs:
```bash
# Scrape by category (recommended)
uv run python -m src.app --site seek --by-category

# Or scrape by specific filter
uv run python -m src.app --site seek
```

3. Copy the generated `database/jobs.db` to your API project root

### Schema Management

Database schema is managed through SQLAlchemy ORM models. Schema changes require:
1. Update models in `src/core/models.py`
2. Restart application (auto-creates missing tables)

For production, consider implementing proper migrations using Alembic.

## Testing

### Run All Tests

```bash
pytest
```

### Run Specific Test File

```bash
pytest tests/test_api.py
pytest tests/test_repository.py
```

### Test Coverage

```bash
pytest --cov=src --cov-report=html
```

### Test Structure

- `test_api.py`: Integration tests for API endpoints (requires `jobs.db` file to exist, skips if missing)
- `test_repository.py`: Unit tests for repository layer (in-memory database)

**Note**: Integration tests in `test_api.py` will automatically skip if `jobs.db` is not found in the project root.

## Development

### Code Style

This project follows:
- PEP 8 style guidelines
- Type hints for all function parameters and returns
- Pylance/Pyright strict type checking
- Clear, descriptive naming conventions
- Minimal but helpful comments

### Dependency Injection

The application uses FastAPI's dependency injection system:
- `get_repository()`: Provides database repository instance to route handlers
- Repository lifecycle managed through application lifespan events

### Error Handling

- 404: Resource not found
- 500: Database initialization or connection errors
- Validation errors automatically handled by Pydantic

## Configuration

### Database URL

Default: `sqlite:///jobs.db`

To change, modify `init_repository()` call in `main.py`:

```python
init_repository(db_url="sqlite:///custom_path.db")
```

### API Settings

Configure in `main.py`:
- `title`: API title
- `description`: API description
- `version`: API version

## Deployment Considerations

### Production Checklist

1. **Database**: Consider PostgreSQL/MySQL for production
2. **Environment Variables**: Externalize configuration (database URL, secrets)
3. **CORS**: Configure CORS middleware if needed for web clients
4. **Rate Limiting**: Add rate limiting for public APIs
5. **Logging**: Configure structured logging
6. **Monitoring**: Add health check endpoint and metrics
7. **Authentication**: Implement API authentication if required
8. **HTTPS**: Use reverse proxy (nginx/Caddy) with SSL

### Performance Optimization

- Current pagination limit: 100 items
- Database indexes recommended on frequently queried fields
- Consider caching for classification/arrangement endpoints
- Connection pooling for multi-worker deployments



