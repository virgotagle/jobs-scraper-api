# Jobs Scraper API

A FastAPI-based REST API for managing and querying job listings scraped from various job sites. This API provides endpoints for searching, filtering, and retrieving detailed job information stored in a SQLite database.

## Data Source 

This API consumes data generated by the [jobs-scraper](https://github.com/virgotagle/jobs-scraper) project - a modern, async web scraper that collects job listings from job sites like Seek.co.nz. The scraper uses Playwright with stealth mode, built-in rate limiting, and stores data in the `jobs.db` SQLite database that this API serves.

**Key features of the scraper:**
- üöÄ Async/await architecture with Playwright browser automation
- üïµÔ∏è Stealth browsing to avoid detection
- ‚è±Ô∏è Respectful rate limiting (2-4 second delays between requests)
- üóÑÔ∏è Automated SQLite storage with the same schema as this API
- üîß Extensible design for adding new job sites

To collect fresh job data, set up and run the jobs-scraper following its [installation guide](https://github.com/virgotagle/jobs-scraper#-installation).

## Architecture

### Technology Stack

- **Framework**: FastAPI 0.122.0+
- **Database**: SQLite with SQLAlchemy 2.0.44+
- **Validation**: Pydantic 2.12.5+
- **Configuration**: Pydantic Settings 2.0.0+
- **Python**: 3.12+
- **Testing**: pytest 8.3.4+

### Project Structure

```
jobs-scraper-api/
‚îú‚îÄ‚îÄ main.py                      # FastAPI application entry point
‚îú‚îÄ‚îÄ pyproject.toml               # Project dependencies and metadata
‚îú‚îÄ‚îÄ .env                         # Environment variables (not in git)
‚îú‚îÄ‚îÄ .env.example                 # Environment variables template
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py          # Database connection management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py        # Custom exception classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py            # SQLAlchemy ORM models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories.py      # Data access layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py           # Pydantic schemas for API
‚îÇ   ‚îî‚îÄ‚îÄ routers/
‚îÇ       ‚îî‚îÄ‚îÄ jobs.py              # Job listing API endpoints
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_api.py              # API endpoint tests (requires jobs.db)
    ‚îî‚îÄ‚îÄ test_repository.py       # Repository layer tests
```

## Data Model

### Job Listings (`job_listings` table)

Primary entity storing job listing information:

- `job_id` (PK): Unique job identifier
- `title`: Job title
- `job_details_url`: URL to full job posting
- `job_summary`: Brief job description
- `company_name`: Employer name
- `location`: Job location
- `country_code`: ISO country code
- `listing_date`: Date job was posted
- `salary_label`: Salary information (optional)
- `work_type`: Employment type (optional)
- `job_classification`: Primary job category (optional)
- `job_sub_classification`: Job subcategory (optional)
- `work_arrangements`: Remote/hybrid/office (optional)

### Job Details (`job_details` table)

Extended job information with 1:1 relationship to listings:

- `job_id` (PK, FK): References `job_listings.job_id`
- `status`: Current job status
- `is_expired`: Expiration flag
- `details`: Full job description
- `is_verified`: Verification status (optional)
- `expires_at`: Expiration timestamp (optional)

## API Endpoints

### Base URL
`/` - API root endpoint returning `{"message": "Jobs Scraper API", "version": "1.0.0"}`

### Job Endpoints (`/jobs`)

#### List Jobs
```
GET /jobs/
```
Query parameters:
- `job_classification` (optional): Filter by job category
- `job_sub_classification` (optional): Filter by subcategory
- `work_arrangements` (optional): Filter by work arrangement
- `skip` (default: 0): Pagination offset (must be ‚â• 0)
- `limit` (default: 100): Results per page (must be 1-1000)

Error responses:
- `400`: Invalid skip or limit values
- `500`: Database error

#### Get Job Details
```
GET /jobs/{job_id}
```
Returns full job information including extended details.

Error responses:
- `404`: Job not found
- `500`: Database error

#### Search Jobs
```
GET /jobs/search
```
Query parameters:
- `keyword` (required): Search term (minimum 2 characters, searches across title, summary, company, location, and details)
- `skip` (default: 0): Pagination offset (must be ‚â• 0)
- `limit` (default: 100): Results per page (must be 1-1000)

Error responses:
- `400`: Invalid keyword, skip, or limit values
- `422`: Missing required keyword parameter
- `500`: Database error

#### Get Classifications
```
GET /jobs/classifications
```
Returns list of all unique job classifications in database.

#### Get Sub-classifications
```
GET /jobs/sub-classifications
```
Returns list of all unique job sub-classifications.

#### Get Work Arrangements
```
GET /jobs/work-arrangements
```
Returns list of all unique work arrangement types.

## Configuration

### Environment Variables

The API uses environment variables for configuration management through Pydantic Settings. Configuration is loaded from a `.env` file in the project root.

#### Available Settings

- `DATABASE_URL`: Database connection string (default: `sqlite:///jobs.db`)

#### Setup Configuration

1. Copy the example environment file:
```bash
cp .env.example .env
```

2. Edit `.env` with your settings:
```bash
# For SQLite (default)
DATABASE_URL=sqlite:///jobs.db

# For PostgreSQL
# DATABASE_URL=postgresql://user:password@localhost/dbname

# For MySQL
# DATABASE_URL=mysql://user:password@localhost/dbname
```

**Important**: The `.env` file is git-ignored and should never be committed. Use `.env.example` as a template.

### Configuration File

Configuration is managed in `src/core/config.py` using Pydantic Settings. To add new configuration options:

1. Add the field to the `Settings` class in `config.py`
2. Add the variable to `.env.example` with documentation
3. Update `.env` with your actual value

## Installation

### Prerequisites

- Python 3.12 or higher
- uv package manager (recommended) or pip

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd jobs-scraper-api
```

2. Install dependencies:
```bash
# Using uv (recommended)
uv sync

# Or using pip
pip install -e .
```

3. Set up configuration:
```bash
# Copy the environment template
cp .env.example .env

# Edit .env with your settings (optional, defaults work for local development)
# DATABASE_URL=sqlite:///jobs.db
```

4. Install development dependencies (already included with uv sync):
```bash
# Development dependencies are automatically installed with uv sync
# To install only production dependencies:
uv sync --no-dev
```

## Running the API

### Development Server

Start the FastAPI development server with auto-reload:

```bash
fastapi dev main.py
```

The API will be available at `http://localhost:8000`

### Production Server

```bash
fastapi run main.py
```

### Interactive API Documentation

Once running, access:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## Database

### Initialization

The database is automatically initialized on application startup using the lifespan context manager in `main.py`. By default, it uses a `jobs.db` SQLite file in the project root.

**Note**: The API expects an existing `jobs.db` file populated by the [jobs-scraper](https://github.com/virgotagle/jobs-scraper) project. If the database doesn't exist or is empty, the API will return empty results.

### Data Collection

To populate the database with job listings:

1. Clone and set up the jobs-scraper:
```bash
git clone https://github.com/virgotagle/jobs-scraper
cd jobs-scraper
uv sync
uv run playwright install
```

2. Run the scraper to collect jobs:
```bash
# Scrape by category (recommended)
uv run python -m src.app --site seek --by-category

# Or scrape by specific filter
uv run python -m src.app --site seek
```

3. Copy the generated `database/jobs.db` to your API project root

### Schema Management

Database schema is managed through SQLAlchemy ORM models. Schema changes require:
1. Update models in `src/core/models.py`
2. Restart application (auto-creates missing tables)

For production, consider implementing proper migrations using Alembic.

## Testing

### Run All Tests

```bash
pytest
```

### Run Specific Test File

```bash
pytest tests/test_api.py
pytest tests/test_repository.py
```

### Test Coverage

```bash
pytest --cov=src --cov-report=html
```

### Test Structure

- `test_api.py`: Integration tests for API endpoints (requires `jobs.db` file to exist, skips if missing)
- `test_repository.py`: Unit tests for repository layer (in-memory database)

**Note**: Integration tests in `test_api.py` will automatically skip if `jobs.db` is not found in the project root.

## Error Handling

The API implements comprehensive error handling with custom exception classes and user-friendly error messages.

### HTTP Status Codes

- **200 OK**: Successful request
- **400 Bad Request**: Invalid input (validation errors, business logic violations)
- **404 Not Found**: Resource not found (job ID doesn't exist)
- **422 Unprocessable Entity**: Request validation error (missing required fields, invalid types)
- **500 Internal Server Error**: Database or server error

### Error Response Format

All errors return JSON with an `error` field:

```json
{
  "error": "Description of what went wrong"
}
```

For validation errors (422), additional details are provided:

```json
{
  "error": "Validation error",
  "details": [
    "field_name: error message"
  ]
}
```

### Custom Exceptions

The API uses three custom exception classes defined in `src/core/exceptions.py`:

- **`JobNotFoundError`**: Raised when a requested job ID doesn't exist (404)
- **`InvalidInputError`**: Raised for business logic validation failures (400)
- **`DatabaseError`**: Raised for database operation failures (500)

### Validation Rules

#### Pagination
- `skip`: Must be a non-negative integer (‚â• 0)
- `limit`: Must be between 1 and 1000

#### Search
- `keyword`: Must be at least 2 characters long

### Example Error Responses

**Invalid limit:**
```bash
curl "http://localhost:8000/jobs?limit=2000"
# Response: {"error": "limit must be between 1 and 1000"}
```

**Job not found:**
```bash
curl "http://localhost:8000/jobs/invalid-id"
# Response: {"error": "Job with ID 'invalid-id' not found"}
```

**Short search keyword:**
```bash
curl "http://localhost:8000/jobs/search?keyword=a"
# Response: {"error": "Search keyword must be at least 2 characters long"}
```

## Development

### Code Style

This project follows:
- PEP 8 style guidelines
- Type hints for all function parameters and returns
- Pylance/Pyright strict type checking
- Clear, descriptive naming conventions
- Minimal but helpful comments

### Dependency Injection

The application uses FastAPI's dependency injection system:
- `get_repository()`: Provides database repository instance to route handlers
- Repository lifecycle managed through application lifespan events

### Error Handling Implementation

The application uses a global exception handler system:
- Custom exceptions are caught and converted to appropriate HTTP responses
- Pydantic validation errors are transformed into user-friendly messages
- Generic exceptions are caught to prevent information leakage
- All database errors return generic 500 responses without exposing internals

See the [Error Handling](#error-handling) section for detailed status codes and response formats.

## Deployment Considerations

### Production Checklist

1. **Database**: Consider PostgreSQL/MySQL for production (update `DATABASE_URL` in `.env`)
2. **Environment Variables**: Set production values in deployment platform (Railway, Render, etc.)
   - Never commit `.env` file
   - Use platform environment variable management
3. **CORS**: Configure CORS middleware if needed for web clients
4. **Rate Limiting**: Add rate limiting for public APIs
5. **Logging**: Configure structured logging
6. **Monitoring**: Add health check endpoint and metrics
7. **Authentication**: Implement API authentication if required
8. **HTTPS**: Use reverse proxy (nginx/Caddy) with SSL or platform SSL
9. **Error Logging**: Consider error tracking service (Sentry, etc.)

### Performance Optimization

- Current pagination limit: 100 items
- Database indexes recommended on frequently queried fields
- Consider caching for classification/arrangement endpoints
- Connection pooling for multi-worker deployments



